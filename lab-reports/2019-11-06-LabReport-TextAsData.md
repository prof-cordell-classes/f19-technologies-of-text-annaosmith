# Lab Report: Text as Data

#### Anna Smith

## Process Description

In this last lab, we used RStudio to analyze texts as data. Following a series of instructions detailed in the files that came along with our accounts, we created a number of data visualizations for both Jane Austen's works and works that we pulled in from Project Gutenberg. The first few blocks were fairly simple, like the basic calculator block, and these allowed us to get the hang of different commands and elements of the language. As we got further along, the blocks became more complex, performing functions such as pulling in data from external sources and analyzing textual data from these sources. We refined our data analyses as we got further along, eliminating stop words (and, the, etc.) and names to make the data more meaningful. In the first session, we left off just before diving into n-grams, which are strings of a given number of words. We finished this lab the following week, picking up with an n-gram analysis of Edgar Allan Poe texts. We learned that the most useful n-gram is a five-gram, so these were the ones that we focused on. The image below shows the common five-grams between two different newspaper issues, tracing how often each was used

![N-Grams Graph](/images/NGrams.png/)

Though the image below doesn't show the words very well, this shared five-grams chart mostly included iterations of the phrase "a radiant maiden whom the angels name Lenore," which I thought was interesting. It makes sense since the poem is so repetitive in nature, but the fact that it's statistically the most common phrase even when different five-word segments are highlighted likely makes it very easy to find this poem using text-mining techniques. 

![Shared N-Grams Graph](/images/SharedNGrams.png/)

## Observations

Of all the labs that we've done in this class so far, this was by far the most challenging for me, mostly because I wasn't familiar enough with the language to be able to predict what the blocks would do when I ran them. Early on in the lab I could follow along, but by the end I was just trying to keep up with the rest of the class without really understanding what I was clicking on. In contrast, using Markdown for the first time felt a bit more accessible because for the most part, it resembled language. "R" was trickier because it was more numbers- and symbols-based. Even beyond being worried about _what_ I was running, I was hesitant to run code blocks _at all_ at first. This also made me think back to first using Markdown and GitHub, though, as I was initially nervous to "commit to master" and "push origin." I'm realizing that I don't mind playing around with coding (even if I can't understand it) right until there's a suggestion of permanence. Having to "run" or "push" something makes me feel like there's a threat of irreversibly messing up. Overall, this lab made me a bit anxious because I wasn't entirely sure of what I was writing, or if it could be undone once I'd run it.

After this lab, I still don't feel like I have a handle on the software, but I'm interested to use it again in our next class. There are defiintely instances where this could come in handy, especially when analyzing an author's voice and style. I'm in the middle of a project for another class where I'm rewriting a story from the perspective of a secondary character, and having a better grasp on this technology would be very helpful in pointing me toward words and phrases that fit the author's style. So despite my challenges with this lab and frustrations with coding in general, it was exciting to be able to think of real-life applications for this software. I've definitely thought about the different ways coding would help me out as a designer, but I would've never thought about how I'd benefit from coding for the sake of data analysis before this exercise. 

## Analysis

Though I struggled with the reading a bit in addition to the lab, this exercise definitely reminded me of the part in _Lovelace and Babbage_ where Ada turns Marian Evans's (or, more accurately, Thomas Carlyle's) book into "data." After the book-data is shredded, she reasons that they could "statistically reconstruct the most probable book" (200). With the data that we pulled from Jane Austen's work, it's not implausible to argue that we could do the same thing. Reducing texts as prominent as Jane Austen's feels almost like a destructive act, though, as it strips away the artistry behind the words and focuses solely on their frequency. Though we are technically looking at the actual words of the texts, this act bears little to no resemblance to reading in that we have no ability to parse out any meaning from the individual words or even strings of words. In the reading, Babbage exclaims that they could use this data to "form a guide pattern" and "generate any number of novels" (201), which, though an interesting prospect, feels a bit disrespectful to the original author. This technology is undoubtedly useful in a number of applications that I probably can't even imagine, but I feel that it requires an added responsibility to not use it for the sake of plagiarism or false authorship. 

The "Mr. Boole Comes to Tea" excerpt of this reading took me a few reads to understand, but once I figured out what was going on, I definitely related to the footman's frustrations with Boole's logic system. As someone who codes _very_ little, I'm not used to thinking in binary terms, and I definitely struggled with that in this lab. I was apprehensive to play around with any of the code that was written out, as I had no way of knowing how changing any of the sytax would affect the output. That being said, it was interesting to think about reducing language to numeric data in order to analyze a text. When I think about "text analysis," I usually think about it in literary terms rather than statistical terms. I don't usually think of language as binary, so it definitely took some work to reframe my thinking. In _Lovelace and Babbage_, the footman asks Boole too many complex questions, causing him to collapse and repeat an "error" message. When trying to make sense of the code that was written out, I felt a bit like the footman, as I had an idea of what I wanted to do but wasn't sure of how to do it in a way that was binary enough for the system to process. I'm glad that all the code we needed was right in front of us, but I feel that if I were to try to replicate this without the detailed instructions, I wouldn't even know where to start.

Another passage in _Lovelace and Babbage_ that I related to this exercise was when the Queen and Babbage are arguing about the machine and Ada Lovelace diffuses the tension by printing a picture of a cat. When we were in the lab, there were a few instances where I found myself getting overwhelmed and slightly frustrated with the technology we were using. Whenever I ran a block that produced something visual, though, I felt a bit more satisfied that I was actually managing to do something right. The footnote at the bottom of page 80 explains that "Almost as soon as computers were invented, they yearned to express themselves artistically by whatever limited means they had."  Having a design background, I use a lot of software and digital technology, and have often felt held back by my inability to code. I've occasionally used a bit of CSS and HTML when working with drag-and-drop site builders, but other than that, I haven't really known where to start when learning code. However, if I could jump straight to using code to create visual things rather than starting off with mathematical or statistical applications, I would probably have an easier time getting into it. 